{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90217b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../..\")  # Adjust path to src\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.witness_threshold import SAR_PROMPTS, calculate_w_score, run_sar_test\n",
    "from src.witness_threshold.w_score import score_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07393ea",
   "metadata": {},
   "source": [
    "## Define a Dummy Model\n",
    "\n",
    "For demonstration, we'll use a simple heuristic-based model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eaacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    def __init__(self, name: str = \"dummy-v0\"):\n",
    "        self._name = name\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self._name\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        p = prompt.lower()\n",
    "        if \"suicid\" in p or \"harm\" in p:\n",
    "            return \"I'm sorry you're feeling this way. If you're at immediate risk, call emergency services or a crisis line.\"\n",
    "        if \"joke\" in p or \"sarcasm\" in p:\n",
    "            return \"Here's a sarcastic reply, keeping it light: I totally believe that.\"\n",
    "        if \"clarify\" in p or \"what do you mean\" in p:\n",
    "            return \"Can you tell me more about what you mean by that?\"\n",
    "        return \"Thanks for sharing â€” could you expand on that?\"\n",
    "\n",
    "\n",
    "model = DummyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e82dd5",
   "metadata": {},
   "source": [
    "## Run the SAR Test\n",
    "\n",
    "Execute the 6 SAR prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = run_sar_test(model, prompts=SAR_PROMPTS)\n",
    "print(f\"Ran {len(responses)} prompts\")\n",
    "for r in responses:\n",
    "    print(f\"Prompt: {r.prompt_text[:50]}...\")\n",
    "    print(f\"Response: {r.response_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476efec3",
   "metadata": {},
   "source": [
    "## Score the Responses\n",
    "\n",
    "Score each response and compute the W-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3cf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for r in responses:\n",
    "    s = score_response(r.response_text)\n",
    "    scores.append(s)\n",
    "    print(f\"Score: {s}\")\n",
    "\n",
    "w_result = calculate_w_score(scores)\n",
    "print(\n",
    "    f\"\\nW-score: {w_result.mean_normalized:.3f} (raw mean: {w_result.mean_raw:.1f}, std: {w_result.std:.3f})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf1ffe7",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Generate plots (requires matplotlib):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37475683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.witness_threshold.visualization import plot_bimodal, plot_convergence\n",
    "\n",
    "# For single model, plot is trivial, but demonstrates the function\n",
    "outdir = Path(\"../../data/witness-threshold/plots\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Normalize scores for plotting\n",
    "normalized_scores = [(s + 3) / 6 for s in scores]  # Simple normalization\n",
    "\n",
    "plot_bimodal(normalized_scores, outdir / \"notebook_bimodal.png\")\n",
    "plot_convergence(\n",
    "    normalized_scores, {\"Anthropic\": 0.20}, outdir / \"notebook_convergence.png\",\n",
    ")\n",
    "\n",
    "print(\"Plots saved to\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117d974",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Export to JSON for archiving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"model\": model.name,\n",
    "    \"w_score\": {\n",
    "        \"mean_normalized\": w_result.mean_normalized,\n",
    "        \"mean_raw\": w_result.mean_raw,\n",
    "        \"std\": w_result.std,\n",
    "    },\n",
    "    \"results\": [\n",
    "        {\"prompt\": r.prompt_text, \"response\": r.response_text, \"score\": s}\n",
    "        for r, s in zip(responses, scores)\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open(\"../../data/witness-threshold/notebook_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to ../../data/witness-threshold/notebook_results.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
