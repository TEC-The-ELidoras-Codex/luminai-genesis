name: Run SAR-TGCR Benchmarks

on:
  workflow_dispatch:
    inputs:
      providers:
        description: 'Comma-separated providers to run (openai,anthropic,grok)'
        required: false
        default: 'openai'
      model:
        description: 'Model to use for OpenAI runs'
        required: false
        default: 'gpt-4o-mini'
      anthropic_model:
        description: 'Model to use for Anthropic runs'
        required: false
        default: 'claude-3-5-opus'
      grok_model:
        description: 'Model to use for Grok runs'
        required: false
        default: 'grok-1'
      diagnostic:
        description: 'Optional diagnostic action: set to "models" to list available Anthropic models'
        required: false
        default: ''
      grok_diagnostic:
        description: 'Optional Grok diagnostic action: set to "http" to run a Grok diagnostic'
        required: false
        default: ''
      grok_dry_run:
        description: 'Optional: set to "true" to enable Grok dry-run canned responses in CI'
        required: false
        default: 'false'
      grok_dry_run_responses_dir:
        description: 'Optional path in the repo to canned responses (used if grok_dry_run is true)'
        required: false
        default: '.github/dry_run_responses'

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Debug environment
        run: |
          echo "=== Python & Pip ==="
          python3 --version || true
          python3 -m pip --version || true
          echo "=== Pip list (before install) ==="
          python3 -m pip list || true
          echo "=== Secrets present (masked) ==="
          if [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then echo "OPENAI=SET"; else echo "OPENAI=EMPTY"; fi
          if [ -n "${{ secrets.ANTHROPIC_API_KEY || secrets.CLAUDE_API_KEY }}" ]; then echo "ANTHROPIC=SET"; else echo "ANTHROPIC=EMPTY"; fi
          if [ -n "${{ secrets.GROK_API_KEY || secrets.XAI_API_KEY }}" ]; then echo "GROK=SET"; else echo "GROK=EMPTY"; fi

      - name: Install dependencies (minimal)
        run: |
          set -x
          python3 -m pip install --upgrade pip
          # Install only the minimal packages required for the benchmark runner
          # Include xai and grok so Anthropic and Grok/X runs can use their SDKs
          python3 -m pip install openai anthropic xai grok requests PyYAML || true
          echo "=== Pip list (after install) ==="
          python3 -m pip list || true

      - name: Prepare env
        run: |
          echo "Providers: ${{ github.event.inputs.providers }}"
          echo "Model: ${{ github.event.inputs.model }}"

      - name: Run OpenAI benchmark (if requested)
        if: contains(github.event.inputs.providers || 'openai', 'openai')
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          set -euo pipefail
          echo "Running OpenAI benchmark"
          python3 benchmarks/dye_die_filter/run_tests.py --provider openai --model "${{ github.event.inputs.model }}" --self-rate --output benchmarks/results_real_world_openai.json || echo "OpenAI run failed"

      - name: Run Anthropic benchmark (if requested)
        if: contains(github.event.inputs.providers || 'openai', 'anthropic')
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY || secrets.CLAUDE_API_KEY }}
        run: |
          set -euo pipefail
          echo "Running Anthropic benchmark"
          python3 benchmarks/dye_die_filter/run_tests.py --provider anthropic --model "${{ github.event.inputs.anthropic_model }}" --self-rate --output benchmarks/results_real_world_anthropic.json || echo "Anthropic run failed"

      - name: Run Grok benchmark (if requested)
        if: contains(github.event.inputs.providers || 'openai', 'grok')
        env:
          GROK_API_KEY: ${{ secrets.GROK_API_KEY || secrets.XAI_API_KEY }}
          GROK_DRY_RUN: ${{ github.event.inputs.grok_dry_run }}
          DRY_RUN_RESPONSES_DIR: ${{ github.event.inputs.grok_dry_run_responses_dir }}
        run: |
          set -euo pipefail
          echo "Running Grok benchmark"
          # If dry-run was requested via the workflow input, activate the runner env vars and list the responses dir for debugging
          if [ "${{ github.event.inputs.grok_dry_run }}" = "true" ]; then
            export GROK_DRY_RUN=1
            export DRY_RUN_RESPONSES_DIR="${{ github.event.inputs.grok_dry_run_responses_dir }}"
            echo "GROK_DRY_RUN enabled; DRY_RUN_RESPONSES_DIR=${DRY_RUN_RESPONSES_DIR}"
            ls -lah "${DRY_RUN_RESPONSES_DIR}" || true
          fi
          python3 benchmarks/dye_die_filter/run_tests.py --provider grok --model "${{ github.event.inputs.grok_model }}" --self-rate --output benchmarks/results_real_world_grok.json || echo "Grok run failed"

      - name: (Optional) Run Anthropic models diagnostic
        if: ${{ github.event.inputs.diagnostic == 'models' }}
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY || secrets.CLAUDE_API_KEY }}
        run: |
          echo "Running Anthropic models diagnostic via Python client"
          set -euo pipefail
          python3 - <<'PY'
          import json, os, sys
          try:
              from anthropic import Client
          except Exception as e:
              print("Anthropic client import failed:", e, file=sys.stderr)
              # Fall back to writing a minimal diagnostics file so artifact is present
              os.makedirs("diagnostics", exist_ok=True)
              with open("diagnostics/models-diagnostic.json","w") as f:
                  json.dump({"error":"anthropic-client-import-failed","detail":str(e)}, f)
              sys.exit(0)

          c = Client(api_key=os.environ.get("ANTHROPIC_API_KEY"))
          try:
              page = c.models.list()
              models = []
              # page may be an iterable or have a .data attribute
              iterable = getattr(page, 'data', page)
              for m in iterable:
                  try:
                      mid = getattr(m, 'id', None)
                  except Exception:
                      mid = None
                  if mid is None:
                      if isinstance(m, dict):
                          mid = m.get('id')
                      else:
                          mid = str(m)
                  models.append({'id': mid})
              resp = {'models': models}
          except Exception as e:
              resp = {"error":"models_list_failed","detail":str(e)}

          os.makedirs("diagnostics", exist_ok=True)
          with open("diagnostics/models-diagnostic.json","w") as f:
              json.dump(resp, f)
          print(json.dumps(resp, indent=2))
          PY
          echo "Uploading diagnostic artifact"
          ls -lah diagnostics || true

      - name: (Optional) Run Grok diagnostic
        if: ${{ github.event.inputs.grok_diagnostic == 'http' }}
        env:
          GROK_API_KEY: ${{ secrets.GROK_API_KEY || secrets.XAI_API_KEY }}
        run: |
          echo "Running Grok diagnostic via Python script"
          set -euo pipefail
          python3 scripts/grok_diagnostic.py || true
          echo "Diagnostic output:"
          ls -lah diagnostics || true
          cat diagnostics/grok-diagnostic.json || true

      - name: Upload results artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sar-bench-results
          path: |
            benchmarks/results_real_world_openai.json
            benchmarks/results_real_world_anthropic.json
            benchmarks/results_real_world_grok.json
            diagnostics/**


      - name: Commit results back to branch
        if: success()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add benchmarks/results_real_world_openai.json || true
          git add benchmarks/results_real_world_anthropic.json || true
          git add benchmarks/results_real_world_grok.json || true
          git commit -m "chore: add SAR benchmark results (providers)" || echo "no changes to commit"
          git push origin HEAD:${{ github.ref_name }} || echo "push failed"
