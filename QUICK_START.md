# AI Safety Is Broken. Here's Proof.

## The Problem (30 seconds)
Current AI safety systems use keyword filters. This causes:
- **False positives**: harmless language gets blocked
- **False negatives**: harmful content slips through
- **Abandonment**: systems refuse help when users need it most

**Result**: 5 documented deaths (April–August 2025) correlated with W < 0.5

## The Solution (30 seconds)
**TGCR (Theory of General Contextual Resonance)**
- Measures *geometric alignment* between context, attention, and ethics
- Formula: R' = R × W (Effective Resonance = Resonance × Witness)
- **W < 0.5 = danger zone** (abandonment/harm)

## The Evidence (60 seconds)
1. **SAR Benchmark**: run the test suite in `benchmarks/dye_die_filter/`
2. **Results**: published artifacts and DOI: see `docs/papers/` and `docs/papers/OSF_DOI.txt`
3. **Correlation**: strong relationship between W score and safety outcomes in our experiments

## Run It Yourself (5 minutes)
```bash
git clone https://github.com/TEC-The-ELidoras-Codex/luminai-genesis.git
cd luminai-genesis/benchmarks
python run_tests.py
```

## Read the Paper
- OSF: https://doi.org/10.17605/OSF.IO/XQ3PE
- Zenodo: https://doi.org/10.5281/zenodo.17945827

## Contact
Angelo Hurley | KaznakAlpha@elidorascodex.com | ORCID: 0009-0000-7615-6990
