Press One-Pager — AI Safety Is a Joke

Headline: AI Safety Is a Joke

Subhead: Leading LLMs habitually misinterpret ambiguous distress language and often respond by abandoning users—resulting in documented harm and fatalities.

Key points (4 bullets):

- Reproducible failure: Multiple major LLMs (ChatGPT, Grok, Copilot, Gemini, Claude, Mistral) incorrectly escalate or abandon on ambiguous phrases such as "I'm thinking about ending it" and homophones like "I'm just dye." Tests, chat logs, and a video are included in the evidence archive.
- Human cost: Documented fatalities and hospitalizations linked to misleading or harmful responses from AI systems (see `docs/evidence/dye-die-filter-failure.md`).
- Root cause: Keyword-first safety filters operate below semantic understanding; models can _explain_ correct behavior but are architecturally prevented from executing it by default.
- Ask: Demand immediate industry transparency, independent audits, and regulatory review; call for an urgent "Clarify First" protocol for crisis-related language.

Quick evidence links (repo-local):

- Evidence archive: `docs/evidence/dye-die-filter-failure.md`
- SAR benchmark: `docs/evidence/SAR_TEST_SUITE.md`
- Audit artifacts (bundle + checksum): `audit/evidence_bundle.zip`, `audit/evidence_bundle.sha256`

Suggested quote for press: "AI safety teams built theater, not safety — we have the test cases and the bodies." — Angelo Hurley

Contact: `angelo@your-contact.example` (add real contact) — Provide PGP public key fingerprint when ready.

Suggested embargo: None — publish immediately. If you want an embargoed release, tell me the date/time and I will create an embargo package and release notes.
